\begin{multicols}{4}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{Probabilistic AI}} \\
\end{center}

\section{Search}
\subsection{Well-defined problem}
Formally defined by four components:
\paragraph{Initial State} The state that the agent starts in.
\paragraph{Sucessor Function} Actions available to agent.
\paragraph{Goal Test} Determines whether agent in goal state.
\paragraph{Path Cost} Fn assigns a numeric cost to each path.

\subsection{Tree Search}
\textbf{function} TREE-SEARCH (\textit{problem, strategy}) \textbf{returns} a solution, or failure\\
initialize search tree using initial state of \textit{problem}\\
\textbf{loop do}\\
    \textbf{if} no candidates for expansion \textbf{then}\\
        \textbf{return} failure\\
    choose leaf node for expansion according to \textit{strategy}\\
    \textbf{if} the node contains a goal state \textbf{then}\\
        \textbf{return} corresponding solution\\
    \textbf{else} expand node and add the resulting nodes to the search tree

\subsection{Measuring performance}
Performance can be measured according to the following four criteria
\paragraph{Completeness} Is algorithm guaranteed to find solution if present
\paragraph{Optimality} Does strategy find the optimal solution
\paragraph{Time complexity} How long taken to find solution
\paragraph{Space complexity} How much memory to perform search

\subsubsection{Measures of problem difficulty}
\textbf{Branching factor} $b$: max. no. of successors to a node, \textbf{depth} $d$ of the shallowest goal node, $m$ maximum length of any path in state space.

\subsection{Breadth-First Search}

Root node expanded first, then all successors of root, then their successors etc.

BFS is complete, but not necessarily optimal (only optimal if path cost nondecreasing function of node depth). Time complexity is in the worst case $b + b^2 + \dots + (b^{d+1} - b) = O(b^{d+1})$. Space complexity is proportional to time complexity as every node must remain in memory (it is either part of fringe or anscestor).

\subsubsection{Uniform cost search}

Variant of BFS in which node with lowest path cost is expanded, not shallowest node.

Completeness only guaranteed if every node has at least cost $\epsilon$, in which case optimality is also guaranteed (path cost increases). Time and space costs are both $O(b^{1+[C^*/\epsilon]})$, where $C^*$ cost of the optimal solution and every action costs at least $\epsilon$.

\subsection{Depth-First Search}

Deepest node in current fringe always expanded first.

DFS is neither complete nor optimal - incomplete because it could get "stuck" going down unbounded tree, not optimal as it may find a solution at a deeper depth than the optimal solution. Worst-case time complexity potentially unbounded, otherwise similar to BFS.
Space complexity reduced as only $bm + 1 = O(bm)$ nodes need to be stored.

\subsubsection{Backtracking search}

Only uses $O(m)$ space, only expands one child of fringe node.

\subsection{Iterative Deepening DFS}

IDFS limits maximimum depth to be expanded to a given limit. Limit increased per full tree expansion.

IDFS is complete for finite branching factor and optimal if path cost nondecreasing function of node depth. Time complexity is $db + (d-1)b + \dots + (1)b^d = O(b^d)$. Space complexity is $O(bd)$ (like DFS).

\subsection{Bidirectional Search}

Run search simltaneously from start to goal and from goal to start, reduces complexity from $b^d$ to $2b^{d/2}$. Requires knowing a goal node, and how to construct tree with goal node as root. Time complexity is $O(b^{d/2}$, as is the space complexity (drawback of bidirectional search).

\subsection{Graph Search}

Like tree search but stores list of already expanded nodes.

\textbf{function} GRAPH-SEARCH (\textit{problem, fringe}) returns a solution, or failure\\
\textit{closed} $\leftarrow$ an empty set\\
\textit{fringe} $\leftarrow$ INSERT(MAKE-NODE(INITIAL-STATE[\textit{problem]}, \textit{fringe})\\
\textbf{loop do}\\
\textbf{if} EMPTY?(\textit{fringe}) \textbf{then}\\
    \textbf{return} failure\\
\textit{node} $\leftarrow$ REMOVE-FIRST(\textit{fringe})\\
\textbf{if} GOAL-TEST[\textit{problem}]STATE[node] \textbf{then}\\
    \textbf{return} SOLUTION(node)\\
\textbf{if} STATE[\textit{node}] is not in \textit{closed} \textbf{then}\\
    add STATE[\textit{node}] to closed\\
    \textit{fringe} $\leftarrow$ INSERT-ALL(EXPAND(node, \textit{problem}), \textit{fringe})\\

\subsection{Best-first Search}

Heuristic function $h(n)$ is estimated cost of cheapest path from node $n$ to goal.

\subsection{A* Search}

Best-first search which combines $g(n)$, the cost to reach a node with $h(n)$ the cost to get from the node to the goal: $f(n) = g(n) + h(n)$.

Using \textbf{tree-search}, A* is optimal if $h(n)$ an admissable heuristic i.e. $h(n)$ doesn't overestimate the actual cost.

Using \textbf{graph-search} admissibility doesn't suffice, we need \textbf{consistency} (also \textbf{monotonicity}).

$h(n)$ is consistent if, for every node $n$ and every successor $n'$ of $n$ generated by any action $a$, the estimated cost of reaching the goal from $n$ is no greater than the step cost of getting to $n'$ plus the estimated cost of reaching the goal from $n'$: $h(n) \leq c(n, a, n') + h(n')$. Consistency of $h(n)$ implies admissibility and nondecreasing $h(n)$ hence the first node selected by consistent graph-search is optimal.

Time and space complexity $O(b^d)$ resp. $O(b^{C*/\epsilon})$ worst-case, often better.

\subsection{IDA* Search}

Iteratively increase maximum $f$-cost ($f(n) = g(n)+h(n)$).

\subsection{Heuristic functions}

\subsubsection{Dominance}

If $h_1$ and $h_2$ admissable and $\forall n \; h_2(n) \geq h_1(n)$ then $h_2$ dominates $h_1$ hence better for search.

Given admissable $h_a$, $h_b$, $h(n) = \max(h_a(n), h_b(n))$ is admissable: dominates $h_a$ and $h_b$.

\subsubsection{Developing heuristics}

Admissibility specifies: $h(n) \leq h^*(n)$, ideally we would use $h(n) = h^*(n)$ - not feasible. Instead we get lower bound by relaxing constraints.

\section{Propositional logic}

\paragraph{Entailment} $\alpha \models \beta$ says if $\alpha$ true then $\beta$ also true
\paragraph{Soundness} An inference algorithm that derives only entailed sentences is called sound or truth-preserving
\paragraph{Completeness} An inference algorithm is complete if it can derive any entailed sentence

\subsection{Syntax}
\begin{tabular}{l l l}
Proposition Symbol	& $P$				& $Q, R, W_{1,3}, ...$ \\
Negation			& $\lnot$			& $\lnot W_{1,3}$ \\
Conjunction			& $\wedge$			& $P \wedge Q$ \\
Disjunction			& $\lor$			& $P \lor Q$ \\
Implication			& $\Rightarrow$		& $P \wedge Q \Rightarrow R$ \\
Biconditional		& $\Leftrightarrow$	& $P \wedge Q \Leftrightarrow R$ \\
\end{tabular}

\subsection{Backus Naur Form}
\begin{tabular}{l c l}
Sentence		& $\rightarrow$	& AtomicSentence \\
				&				& | ComplexSentence \\
AtomicSentence	& $\rightarrow$	& True | False | Symbol \\
Symbol			& $\rightarrow$	& P | Q | R | \dots \\
ComplexSentence & $\rightarrow$	& $\lnot$Sentence \\
				&				& | (Sentence $\land$ Sentence) \\
\end{tabular}
\paragraph{Precedence} $\lnot, \land, \lor, \Rightarrow, \Leftrightarrow$

\subsection{Somthing smart here}

$\alpha \Leftrightarrow \beta$ iff $\alpha \models \beta$ and $\beta \models \alpha$

\paragraph{Deduction theorem} for sentences $\alpha$ and $\beta$, $\alpha \models \beta$ iff. $\alpha \Rightarrow \beta$
\paragraph{Satisfiability of sentence} $\Rightarrow$ true in some model
\paragraph{Proof by contradiction} $\alpha \models \beta$ iff. $(\alpha \land \lnot \beta)$ unsatisfiable
\paragraph{Modus ponens}
$\frac{\alpha_1, \alpha_2, \dots, \alpha_k \;\; \alpha_1 \land \alpha_2 \land \dots \land \alpha_k \Rightarrow \beta}{\beta}$ or more simply: if $\alpha \Rightarrow \beta$ and $\alpha$ then $\beta$
\paragraph{Unit resolution}

$\frac{\ell_1 \lor \dots \lor \ell_k, m}{\ell_1 \lor \dots \lor \ell_{i-1} \lor \ell_{i+1} \lor \dots \lor \ell_{k}}$, where $\ell_i$ and $m$ are negation of one another \\
example: $(P \lor Q) \land (\lnot P \lor R) = (Q \lor R)$

\subsection{Conjunctive normal form}
$\ell_{1,1} \lor \dots \lor \ell_{1,k} \land \dots \land (\ell_{n,1} \lor \dots \lor \ell_{n,k})$

\paragraph{Recipe for CNF}
\begin{enumerate}
\item eliminate "$\Leftrightarrow$": $\alpha \Leftrightarrow \beta \rightarrow (\alpha \Rightarrow \beta) \land (\beta \Rightarrow \alpha)$
\item eliminate "$\Rightarrow$": $\alpha \Rightarrow \beta \rightarrow \lnot \alpha \lor \beta$
\item move $\lnot$ inwards:
	\begin{itemize}
	\item $\lnot(\lnot \alpha) \equiv \alpha$
	\item $\lnot(\alpha \land \beta) \equiv (\lnot \alpha \lor \lnot \beta)$
	\item $\lnot(\alpha \lor \beta) \equiv (\lnot \alpha \land \lnot \beta)$
	\end{itemize}
\end{enumerate}

\subsection{Resolution Algorithm}

Inference based on resolution uses proof by contradiction to show $KB \models \alpha$ we show $KB \land \lnot \alpha$ unsatisfiable.

\begin{enumerate}
\item Convert $KB \land \lnot \alpha$ to CNF
\item Apply reolution to resulting clauses
\end{enumerate}

Eventually either no new clauses can be added $\Rightarrow$ $KB \not\models \alpha$ or two clauses resolve to yield empty clause $\Rightarrow$ $KB \models \alpha$

\section{First Order Logic}
\subsection{Syntax}
\begin{tabular}{l l l}
Sentence		& $\rightarrow$	& AtomicSentence |\\
				&				& (Sentence Connective Sentence) |\\
				&				& Quantifier, Variable, ... Sentence |\\
				&				& Sentence \\
AtomicSentence	& $\rightarrow$	& Predicate(Term,...) |\\
				&				& Term = Term \\
Term			& $\rightarrow$	& Function(Term) | Constant |\\
				&				& Variable \\
Connective		& $\rightarrow$	& $\Rightarrow$ | $\land$ | $\lor$ | $\Leftrightarrow$ \\
Quantifier		& $\rightarrow$	& $\forall$ | $\exists$ \\
Constant		& $\rightarrow$	& $A$ | $X_1$ | John | \dots \\
Variable		& $\rightarrow$	& $a$ | $x$ | $s$ | \dots \\
Predicate		& $\rightarrow$	& Before | HasColor | \dots \\
Function		& $\rightarrow$	& Mother | LeftLeg | \dots \\
\end{tabular}

\section{Probabilities}

\subsection{Axioms}
\begin{enumerate}
\item $0 \geq P(a) \leq 1$
\item $P(true) = 1$, $P(false) = 0$
\item $P(a \lor b) = P(a) + P(b) - P(a \land b)$
\end{enumerate}

\subsection{Conditional Probability}
$P(a|b) = \frac{P(a \land b)}{P(b)} \equiv P(a \land b) = P(a|b)P(b) \equiv P(a \land b) = P(b|a)P(a)$

Probability of a proposition is equal to sum of probablilities of atomic events in which it holds ($a(a)$).
$P(a) = \sum_{e_i \in e(a)}{P(e_i)}$

\paragraph{Marginalisation} summing out the variables which one is not interested in to obtain unconditional probability: $P(Y) = \sum_z P(Y,z)$

\paragraph{Conditioning} marginalisation with conditional probabilities: $P(Y) = \sum_z P(Y|z)P(z)$

\paragraph{Normalisation} with $X$ the query variable (cavity), $E$ set of evidence variables (Toothache), $e$ observed values, $Y$ unobserved variables (catch): $P(X|e) = \alpha P(X,e) = \alpha \sum_y P(X,e,y)$

\paragraph{Independence} between propositions $a$, $b$ expressed as $a \bot b \Rightarrow P(a|b) = P(a)$ or $P(b|a) = P(b)$ or $P(a \land b) = P(a)P(b)$

\paragraph{Bayes' Rule} $P(b|a) = \frac{P(a|b)P(b)}{P(a)}$ \\
with normalisation: $P(Y|X) = \alpha P(X|Y)P(Y)$, $\alpha$ such that entries $P(Y|X)$ sum to 1

\paragraph{Conditional Independence} of $X$ and $Y$ given $Z$\\
$P(X,Y|Z) = P(X|Z) P(Y|Z)$\\
$P(X|Y,Z) = P(X|Z)$ and $P(Y|X,Z) = P(Y|Z)$

\section{Probabilistic reasoning over time}

\subsection{Markov models}
Assume that current state dependent on subset of prior states (first-oder on previous)
\paragraph{First-order} $P(X_t|X_{0:t-1}) = P(X_t,|X_{t-1})$
\paragraph{Second-order} $P(X_t|X_{0:t-1}) = P(X_t,|X_{t-2},X_{t-1})$
\paragraph{Sensor assumption} $P(E_t|X_{0:t},E_{0:t-1}) = P(E_t, X_t)$
\paragraph{Complete joint distribution}
$P(X_{0:t},E_{1:t}) = P(X_0) \prod\limits_{i=1}^t P(X_i|X_i-1)P(E_i|X_i)$
\subsection{Inference in Temporal models}
\paragraph{Filtering} computation of the \textbf{belief state}, also called state estimation. Wish to compute $P(X_t|e_{1:t})$
\paragraph{Prediction} computation of posterior distribution over future state. $P(X_{t+k}|e_{1:t}) \; , \; k>0$ \\
$P(X_{t+1}|e_{1:t+1}) = \alpha P(e_{t+1}|X_{t+1}) \sum\limits_{x_t}P(X_{t+1}|x_t)P(x_t|e_{1:t})$ \\
$P(X_{t+k+1}|e_{1:t}) = \sum\limits_{x_{t+k}}P(X_{t+k+1}|x_{t+k})P(x_{t+k}|e_{1:t})$
\paragraph{Smoothing} computation of posterior distribution over past state. $\bm{P}(\bm{X}_k|\bm{e}_{1:t}) \; , \; 0 \leq k < t$ \\
$\bm{P}(\bm{X}_k|\bm{e}_{1:t}) = \alpha \bm{P}(\bm{X}_k|\bm{e}_{1:k})\bm{P}(\bm{e}_{k+1:t}|\bm{X}_k)$ \\
$\bm{P}(\bm{e}_{k+1:t}|\bm{X}_k) = \sum\limits_{\bm{x}_{k+1}}P(\bm{e}_{k+1}|\bm{x}_{k+1})P(\bm{e}_{k+2:t},\bm{x}_{k+1})\bm{P}(\bm{x}_{k+1}|\bm{X}_k)$
\paragraph{Most likely explanation} sequence of states most likely to have generated given observations $e_{1:t}$. $\arg\!\max_{x_{1:t}} P(X_{1:t}|e_{1:t})$ \\
Viterbi algorithm: $\max_{x_{1:t}} \bm{P}(\bm{x}_{1:t},\bm{X}_{1+t}|\bm{e}_{1:t+1}) = \alpha \bm{P}(\bm{e}_{t+1}|\bm{X}_{t+1})\max\limits_{\bm{x}_t}\left(\bm{P}(\bm{X}_{t+1}|\bm{x}_t) \max\limits_{\bm{x}_{1:t-1}} P(\bm{x}_{1:t} | \bm{e}_{1:t}) \right)$
\paragraph{Learning} transition and sensor models can be learned from observations.

\subsection{Kalman Filters}
TBD

\subsection{Dynamic Bayesian Networks}

TBD

\end{multicols}
